{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlK289O15+ohVimX7NUL9A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshayjain777/CS584-Final-Project/blob/main/Image_Colorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqXmTs16_GQj",
        "outputId": "1a19e91d-acbf-4900-a977-84068fe8e9ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/ImageColorization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwEjBsi5_QOJ",
        "outputId": "83d8ca8c-c3f9-4b79-af44-c38268bdee05"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ImageColorization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4OlsuFX_2PI",
        "outputId": "ff028cf8-ac1d-48d0-fe21-6493a458cacc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " code.py  'deep-koalarization-master 2'   images        README.md\n",
            " data\t   download.py\t\t\t  __pycache__\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iJt2KNy9-qLk",
        "outputId": "d7a3151f-5a18-45af-a367-8ac85d584958"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error\n",
            "error\n",
            "error\n",
            "error\n",
            "error\n",
            "error\n",
            "error\n",
            "(0, 1)\n",
            "(0,)\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_52 (Conv2D)          (None, 128, 128, 64)      640       \n",
            "                                                                 \n",
            " conv2d_53 (Conv2D)          (None, 128, 128, 128)     73856     \n",
            "                                                                 \n",
            " conv2d_54 (Conv2D)          (None, 64, 64, 128)       147584    \n",
            "                                                                 \n",
            " conv2d_55 (Conv2D)          (None, 64, 64, 256)       295168    \n",
            "                                                                 \n",
            " conv2d_56 (Conv2D)          (None, 32, 32, 256)       590080    \n",
            "                                                                 \n",
            " conv2d_57 (Conv2D)          (None, 32, 32, 512)       1180160   \n",
            "                                                                 \n",
            " conv2d_58 (Conv2D)          (None, 32, 32, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_59 (Conv2D)          (None, 32, 32, 256)       1179904   \n",
            "                                                                 \n",
            " conv2d_60 (Conv2D)          (None, 32, 32, 128)       295040    \n",
            "                                                                 \n",
            " up_sampling2d_12 (UpSamplin  (None, 64, 64, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_61 (Conv2D)          (None, 64, 64, 64)        73792     \n",
            "                                                                 \n",
            " up_sampling2d_13 (UpSamplin  (None, 128, 128, 64)     0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_62 (Conv2D)          (None, 128, 128, 32)      18464     \n",
            "                                                                 \n",
            " conv2d_63 (Conv2D)          (None, 128, 128, 16)      4624      \n",
            "                                                                 \n",
            " conv2d_64 (Conv2D)          (None, 128, 128, 2)       290       \n",
            "                                                                 \n",
            " up_sampling2d_14 (UpSamplin  (None, 256, 256, 2)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,219,410\n",
            "Trainable params: 6,219,410\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-1f148e405803>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'colorize_autoencoder.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1501\u001b[0m         \u001b[0;34m\"`validation_split={validation_split}`. Either provide more data, or a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m         \"different value for the `validation_split` argument.\" .format(\n\u001b[0;32m-> 1503\u001b[0;31m             batch_dim=batch_dim, validation_split=validation_split))\n\u001b[0m\u001b[1;32m   1504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Training data contains 0 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.1`. Either provide more data, or a different value for the `validation_split` argument."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from keras.layers import Conv2D, UpSampling2D\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import img_to_array,load_img\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imsave\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#import tensorflow.compat.v2 as tf\n",
        "#print(\"tf version\",tf.__version__)\n",
        "\n",
        "\n",
        "folder_path = 'images/'\n",
        "\n",
        "# #Normalize images - divide by 255\n",
        "# train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "\n",
        "# #Resize images, if needed\n",
        "# train = train_datagen.flow_from_directory(path, \n",
        "#                                         target_size=(256, 256), \n",
        "#                                         class_mode=None)\n",
        "\n",
        "\n",
        "train = os.listdir(folder_path)\n",
        "\n",
        "#Convert from RGB to Lab\n",
        "\"\"\"\n",
        "by iterating on each image, we convert the RGB to Lab. \n",
        "Think of LAB image as a grey image in L channel and all color info stored in A and B channels. \n",
        "The input to the network will be the L channel, so we assign L channel to X vector. \n",
        "And assign A and B to Y.\n",
        "\"\"\"\n",
        "\n",
        "X =[]\n",
        "Y =[]\n",
        "for img in train[0]:\n",
        "  try:\n",
        "      print(\"hello\",img.shape)\n",
        "      lab = rgb2lab(img)\n",
        "      print(\"hello\")\n",
        "      X.append(lab[:,:,0]) \n",
        "      Y.append(lab[:,:,1:] / 128) #A and B values range from -127 to 128, \n",
        "      #so we divide the values by 128 to restrict values to between -1 and 1.\n",
        "  except:\n",
        "     print('error')\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "X = X.reshape(X.shape+(1,)) #dimensions to be the same for X and Y\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "\n",
        "#Encoder\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2, input_shape=(256, 256, 1)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(128, (3,3), activation='relu', padding='same', strides=2))\n",
        "model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3,3), activation='relu', padding='same', strides=2))\n",
        "model.add(Conv2D(512, (3,3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(512, (3,3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
        "\n",
        "#Decoder\n",
        "#Decoder\n",
        "#Note: For the last layer we use tanh instead of Relu. \n",
        "#This is because we are colorizing the image in this layer using 2 filters, A and B.\n",
        "#A and B values range between -1 and 1 so tanh (or hyperbolic tangent) is used\n",
        "#as it also has the range between -1 and 1. \n",
        "#Other functions go from 0 to 1.\n",
        "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(16, (3,3), activation='relu', padding='same'))\n",
        "model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.compile(optimizer='adam', loss='mse' , metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.fit(X,Y,validation_split=0.1, epochs=5, batch_size=16)\n",
        "\n",
        "model.save('colorize_autoencoder.model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fXVxSHOC_E-I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}